# Anatomy of Isaac Lab

# 1. Objectives

In [the previous article](./S04E02_train_unitree_go2_with_isaac_lab.md), we discussed how to use Isaac Lab for simulation training of the Unitree Go2 robotic dog.

After the simulation training, a set of model parameters was generated, as known as "checkpoint". 

Then, using the model loaded with checkpoint, a Unitree Go2 robotic dog can walk freely in the simulated environment, even when subjected to external forces for pushing and pulling (simulated by quickly moving the computer mouse). If the external force is not too strong, the Unitree Go2 can keep balanced when walking in a flat or a rough ground.

Our ultimate goal is to train the robotic dog in a simulated environment to generate a checkpoint of the motion model, and then apply the motion model loaded with the checkpoint to the physical robotic dog, to enable it to walk stably in the real world, in short, our goal is "sim-to-real".

In the previous article, we trained a motion model checkpoint for the robotic dog in the Isaac Lab simulation environment. Our next task is divided into two major steps:

1. Referring to [the official tutorial of Gymnasium](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/), we will write python programs to implement our own environment `env`, where the observation input and the action output do not come from the Isaac Lab simulation environment, but from the physical Unitree-Go2 robotic dog.
   
2. Referring to [the official tutorial of the Unitree robotic dog Go2](https://support.unitree.com/home/en/developer/Basic_motion_control), we will call the basic motion control APIs to receive the real-time motion status of the robotic dog, and then convert it into observations for the `env`.
   
    After then, we will use these observations as inputs for the motion model, after that convert the outputs generated by the motion model into actions, to control the robotic dog's next movements in real time.

&nbsp;
Since there are quite some content involved in these 2 tasks, we focus the subject of this article on the implementation of the first step mentioned above: to implement our own environment `env`. In the next article, we will elaborate the second step.

Specifically, the first task consists of the following sub-tasks:

1. Read the source code of Isaac Lab to obtain the range of values for observations and actions in the simulation environment, that is, `observation_space` and `action_space`.
   
2. Since the data types for observations and actions are tensors, it is necessary to understand the physical meaning of each element in the tensors, more specifically, each tensor element corresponds to which joint's what parameter of the robotic dog.
   
3. Read the source code of Isaac Lab to understand how to initialize the motion model and how to load the checkpoint.

As Isaac Lab is under rapid iterative development, for the time being, [its official tutorials](https://isaac-sim.github.io/IsaacLab/source/tutorials/index.html) are neither comprehensive nor detailed enough.

Therefore, if you want to understand Isaac Lab in depth, for now, the only way is to read its source code. 

&nbsp;
# 2. Observation space & action space

## 2.1 play.py

As mentioned in [the previous article](./S04E02_train_unitree_go2_with_isaac_lab.md), when we use the model's checkpoint generated from simulation training, to control a unitree go2 robotic dog's 3D model walking stably in the simulated environment, we can execute the following command,

~~~
$ cd ${HOME}/IsaacLab

$ ./isaaclab.sh -p source/standalone/workflows/rsl_rl/play.py --task Isaac-Velocity-Flat-Unitree-Go2-v0 --num_envs 1
~~~

We can add `print` in the `play.py` code, to display the content of `observations` and `actions`. 

~~~
from rsl_rl.runners import OnPolicyRunner
import omni.isaac.lab_tasks  # noqa: F401
from omni.isaac.lab_tasks.utils import get_checkpoint_path, parse_env_cfg
from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
    RslRlOnPolicyRunnerCfg,
    RslRlVecEnvWrapper,
    ...
)

def main():
    """Play with RSL-RL agent."""
    # parse configuration
    env_cfg = parse_env_cfg(
        args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric
    )
    agent_cfg: RslRlOnPolicyRunnerCfg = cli_args.parse_rsl_rl_cfg(args_cli.task, args_cli)

    # create isaac environment
    # args_cli.task == "Isaac-Velocity-Flat-Unitree-Go2-v0"
    env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)
    
    # wrap around environment for rsl-rl
    env = RslRlVecEnvWrapper(env)

    ppo_runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=None, device=agent_cfg.device)
    ppo_runner.load(resume_path)
    # obtain the trained policy for inference
    policy = ppo_runner.get_inference_policy(device=env.unwrapped.device)
    
    # reset environment
    obs, _ = env.get_observations()
    timestep = 0
    # simulate environment
    while simulation_app.is_running():
        # run everything in inference mode
        with torch.inference_mode():
            # agent stepping
            actions = policy(obs)
            print(f"\n 【Kan】actions: {actions}\n")

            # env stepping
            obs, _, _, _ = env.step(actions)
            print(f"\n 【Kan】obs: {obs}")
            
    # close the simulator
    env.close()
~~~

1. `play.py` locates in `${HOME}/IsaacLab/source/standalone/workflows/rsl_rl/play.py`

2. `actions` is a list, with 12 elements; `observations` is also a list, with 48 elements. For example,

~~~
$ ./isaaclab.sh -p source/standalone/workflows/rsl_rl/play.py --task Isaac-Velocity-Flat-Unitree-Go2-Play-v0 --num_envs 1

 【Kan】actions: tensor([[ 0.8928, -0.4638,  0.2429, -1.2878, -1.2111, -1.2118,  0.8140,  0.8757,
         -0.9861, -0.4190,  0.1238, -0.6535]], device='cuda:0')
 
 【Kan】obs: tensor([[ 3.3607e-01,  5.1930e-01,  5.0650e-02, -8.9403e-04, -3.5218e-01,
         -5.9714e-01,  4.8754e-03, -2.0455e-02, -9.9978e-01,  3.0070e-01,
          4.8536e-01, -5.9996e-01,  3.2151e-01, -5.0676e-01,  4.1084e-01,
         -4.9038e-01, -3.7618e-02, -1.9251e-01,  1.1657e-01,  1.6681e-01,
         -3.3381e-01, -1.0723e-01, -1.1087e-01,  8.0957e-02, -1.3078e+00,
         -8.5041e-01, -7.8278e-01, -9.9005e-01,  2.8531e+00, -1.9452e-01,
          1.9639e+00, -1.2273e+00, -1.8131e+00,  2.1571e+00, -9.7164e-01,
          5.4120e-01, -2.2933e-01, -7.3102e-01,  1.2406e+00, -1.2660e+00,
         -4.7365e-01, -9.7907e-01,  2.3704e-01,  1.7186e-01, -6.7613e-01,
         -9.9902e-02, -1.6771e-01,  2.2881e-01]], device='cuda:0')        
~~~

Although we have got samples of `observations` and `actions`, we have not yet got their value ranges, `observation_space` and `action_space`.

&nbsp;
## 2.2 rsl_rl.py

Referring to the source code of `play.py`, we know that `observations` come from `env`, and `actions` come from `policy`.

~~~
from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
    RslRlVecEnvWrapper, ...
)
    
env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)
env = RslRlVecEnvWrapper(env)
obs, _ = env.get_observations()

ppo_runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=None, device=agent_cfg.device)
ppo_runner.load(resume_path)
policy = ppo_runner.get_inference_policy(device=env.unwrapped.device)
actions = policy(obs)
~~~

Next, we will read into the source code related to `RslRlVecEnvWrapper`, to see if there is any information related to `observation_space` over there.

In case `RslRlVecEnvWrapper` does not have information related to `observation_space`, then we will continue to review the source code of `gym.make()`.

Referring to the source code of `play.py`, we know that `RslRlVecEnvWrapper` is related to `wrappers/rsl_rl/__init__.py`. Tracing back to the ancestor classes of `wrappers/rsl_rl/__init__.py`, we eventually find that the source code for `RslRlVecEnvWrapper` is located in `rsl_rl/vecenv_wrapper.py`.

~~~
from rsl_rl.env import VecEnv

class RslRlVecEnvWrapper(VecEnv):
    @property
    def observation_space(self) -> gym.Space:
        """Returns the :attr:`Env` :attr:`observation_space`."""
        return self.env.observation_space

    @property
    def action_space(self) -> gym.Space:
        """Returns the :attr:`Env` :attr:`action_space`."""
        return self.env.action_space
        
    def get_observations(self) -> tuple[torch.Tensor, dict]:
        """Returns the current observations of the environment."""
        if hasattr(self.unwrapped, "observation_manager"):
            obs_dict = self.unwrapped.observation_manager.compute()
        else:
            obs_dict = self.unwrapped._get_observations()
        
        return obs_dict["policy"], {"observations": obs_dict}
~~~

1. `rsl_rl/vecenv_wrapper.py` locates in `/home/robot/IsaacLab/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/utils/wrappers/rsl_rl/vecenv_wrapper.py`,

2. Luckily, `rsl_rl/vecenv_wrapper.py` contains the source code of both `observation_space` and `action_space`.

&nbsp;
## 2.3 observation_space & action_space

We added two `print` into the source code of `play.py`, to display the content of `observation_space` and `action_space`. 

The first sub-task, finding the range of values for `observation_space` and `action_space`, is done. 

~~~
from rsl_rl.runners import OnPolicyRunner
import omni.isaac.lab_tasks  # noqa: F401
from omni.isaac.lab_tasks.utils import get_checkpoint_path, parse_env_cfg
from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
    RslRlOnPolicyRunnerCfg,
    RslRlVecEnvWrapper,
    ...
)

def main():
    """Play with RSL-RL agent."""
    # parse configuration
    env_cfg = parse_env_cfg(
        args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric
    )
    agent_cfg: RslRlOnPolicyRunnerCfg = cli_args.parse_rsl_rl_cfg(args_cli.task, args_cli)

    # create isaac environment
    # args_cli.task == "Isaac-Velocity-Flat-Unitree-Go2-v0"
    env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)
    
    # wrap around environment for rsl-rl
    env = RslRlVecEnvWrapper(env)

    ppo_runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=None, device=agent_cfg.device)
    ppo_runner.load(resume_path)
    # obtain the trained policy for inference
    policy = ppo_runner.get_inference_policy(device=env.unwrapped.device)
    
    # reset environment
    obs, _ = env.get_observations()
    timestep = 0
    # simulate environment
    while simulation_app.is_running():
        # run everything in inference mode
        with torch.inference_mode():
            # agent stepping
            actions = policy(obs)
            # env stepping
            obs, _, _, _ = env.step(actions)
            
            print(f"\n【Kan】observation_space: '{env.observation_space}'")
            print(f"【Kan】action_space: '{env.action_space}' \n")
            
    # close the simulator
    env.close()
~~~

Following is the execution result,

~~~
$ ./isaaclab.sh -p source/standalone/workflows/rsl_rl/play.py --task Isaac-Velocity-Flat-Unitree-Go2-Play-v0 --num_envs 1

【Kan】observation_space: 'Dict('policy': Box(-inf, inf, (1, 48), float32))'
【Kan】action_space: 'Box(-inf, inf, (1, 12), float32)' 
~~~

&nbsp;
# 3. The physical meanings of observations

## 3.1 manager_based_rl_env.py

Referring to the source code of `rsl_rl/vecenv_wrapper.py`, `observations` come from `self.unwrapped.observation_manager.compute()`. 

~~~
    def get_observations(self) -> tuple[torch.Tensor, dict]:
        """Returns the current observations of the environment."""
        if hasattr(self.unwrapped, "observation_manager"):
            print(f"\n【Kan】 type(self.unwrapped): \n\t'{type(self.unwrapped)}')\n")
            obs_dict = self.unwrapped.observation_manager.compute()
        else:
            obs_dict = self.unwrapped._get_observations()
        
        return obs_dict["policy"], {"observations": obs_dict}
~~~

To understand this piece of code in more depth, we need to know what class `self.unwrapped` is of. Hence, we added a `print` in the source code. 

`self.unwrapped` is an instance of `ManagerBasedRLEnv` class. 

~~~
$ ./isaaclab.sh -p source/standalone/workflows/rsl_rl/play.py --task Isaac-Velocity-Flat-Unitree-Go2-Play-v0 --num_envs 1

【Kan】 type(self.unwrapped):
    '<class 'omni.isaac.lab.envs.manager_based_rl_env.ManagerBasedRLEnv'>')
~~~

&nbsp;
## 3.2 observation_manager.py

Starting from the source code of `manager_based_rl_env.py`, we traced back its ancestor classes, and found that we needed to read carefully the source code of `observation_manager`. 

~~~
class ObservationManager(ManagerBase):
        
    def compute(self) -> dict[str, torch.Tensor | dict[str, torch.Tensor]]:
        # create a buffer for storing obs from all the groups
        obs_buffer = dict()
        # iterate over all the terms in each group
        for group_name in self._group_obs_term_names:
            obs_buffer[group_name] = self.compute_group(group_name)
        # otherwise return a dict with observations of all groups
        return obs_buffer
        
    def compute_group(self, group_name: str) -> torch.Tensor | dict[str, torch.Tensor]:
        # iterate over all the terms in each group
        group_term_names = self._group_obs_term_names[group_name]
        print(f"\n 【Kan】group_term_names: {group_term_names}")
        
        # buffer to store obs per group
        group_obs = dict.fromkeys(group_term_names, None)
        # read attributes for each term
        obs_terms = zip(group_term_names, self._group_obs_term_cfgs[group_name])

        # evaluate terms: compute, add noise, clip, scale, custom modifiers
        for name, term_cfg in obs_terms:
            # compute term's value
            obs: torch.Tensor = term_cfg.func(self._env, **term_cfg.params).clone()
            print(f" 【Kan】obs: {obs}")
            
            # apply post-processing
            if term_cfg.modifiers is not None:
                for modifier in term_cfg.modifiers:
                    obs = modifier.func(obs, **modifier.params)
            if term_cfg.noise:
                obs = term_cfg.noise.func(obs, term_cfg.noise)
            if term_cfg.clip:
                obs = obs.clip_(min=term_cfg.clip[0], max=term_cfg.clip[1])
            if term_cfg.scale:
                obs = obs.mul_(term_cfg.scale)
            # add value to list
            group_obs[name] = obs
            
        print(f" 【Kan】group_obs: {group_obs}\n")
        # concatenate all observations in the group together
        if self._group_obs_concatenate[group_name]:
            return torch.cat(list(group_obs.values()), dim=-1)
        else:
            return group_obs               
~~~

The source code of `observation_manager` locates at `/home/robot/IsaacLab/source/extensions/omni.isaac.lab/omni/isaac/lab/managers/observation_manager.py`. 

We executed `play.py` again, to print out the content of `group_obs`. 

~~~
$ ./isaaclab.sh -p source/standalone/workflows/rsl_rl/play.py --task Isaac-Velocity-Flat-Unitree-Go2-Play-v0 --num_envs 1

 【Kan】group_term_names: [
        'base_lin_vel', 'base_ang_vel', 'projected_gravity', 
        'velocity_commands', 'joint_pos', 'joint_vel', 'actions']
 【Kan】obs: tensor([[-0.4864, -0.7890,  0.0071]], device='cuda:0')
 【Kan】obs: tensor([[-0.5989,  0.0707, -0.0675]], device='cuda:0')
 【Kan】obs: tensor([[ 0.0087, -0.0022, -1.0000]], device='cuda:0')
 【Kan】obs: tensor([[-0.5736, -0.8130,  0.0198]], device='cuda:0')
 【Kan】obs: tensor([[-0.1097,  0.2212,  0.1241, -0.1258,  0.4233, -0.3994,  0.0043, -0.2238,
         -0.3595, -0.3035, -0.2136, -0.2348]], device='cuda:0')
 【Kan】obs: tensor([[-3.1006,  1.2976, -0.0911,  3.1422,  0.3950,  0.1218,  1.1565, -0.5976,
         -2.6858, -0.1952, -0.2954, -2.3376]], device='cuda:0')
 【Kan】obs: tensor([[-1.2007,  0.1980,  0.1681,  0.8183,  1.4110, -0.2133,  0.4731, -1.4501,
         -1.1923, -0.0632, -0.5582,  0.3421]], device='cuda:0')
 【Kan】group_obs: {
        'base_lin_vel': tensor([[-0.5578, -0.8615, -0.0395]], device='cuda:0'), 
        'base_ang_vel': tensor([[-0.5672,  0.1676, -0.0858]], device='cuda:0'), 
        'projected_gravity': tensor([[ 0.0587,  0.0334, -1.0118]], device='cuda:0'), 
        'velocity_commands': tensor([[-0.5736, -0.8130,  0.0198]], device='cuda:0'), 
        'joint_pos': tensor([[-0.1076,  0.2249,  0.1174, -0.1215,  0.4330, -0.4039, -0.0043, -0.2249,
         -0.3630, -0.2957, -0.2046, -0.2444]], device='cuda:0'), 
        'joint_vel': tensor([[-3.6872,  0.8345,  0.9783,  3.4316,  0.9197,  0.7776, -0.0279, -1.5330,
         -2.3675,  0.7889, -1.4047, -3.2064]], device='cuda:0'), 
        'actions': tensor([[-1.2007,  0.1980,  0.1681,  0.8183,  1.4110, -0.2133,  0.4731, -1.4501,
         -1.1923, -0.0632, -0.5582,  0.3421]], device='cuda:0')}
~~~

Observation tensor consists of 48 elements, corresponding to 7 physical parameters, like `base_lin_vel`. 

So far, the 2'nd sub-task, to understand the physical meaning of each element of observations, is done. 

&nbsp;
# 4. The physical meanings of actions

## 4.1 env.step()

Referring to the source code of `play.py`, `actions` come from `policy`, and `policy` comes from `ppo_runner`. 

However, as the output of `policy`, `actions = policy(obs)`, all we know about `action` so far is that it is a tensor, but we don't know yet each element of the `action` tensor corresponds to what physical parameter of the unitree go2 robotic dog. 

Therefore, we reviewed the source code of `env.step(actions)`, to see how to use `action` tensor. 

~~~
from rsl_rl.runners import OnPolicyRunner
import omni.isaac.lab_tasks  # noqa: F401
from omni.isaac.lab_tasks.utils import get_checkpoint_path, parse_env_cfg
from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
    RslRlOnPolicyRunnerCfg,
    RslRlVecEnvWrapper,
    ...
)

def main():
    """Play with RSL-RL agent."""
    # parse configuration
    env_cfg = parse_env_cfg(
        args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric
    )
    agent_cfg: RslRlOnPolicyRunnerCfg = cli_args.parse_rsl_rl_cfg(args_cli.task, args_cli)

    # create isaac environment
    # args_cli.task == "Isaac-Velocity-Flat-Unitree-Go2-v0"
    env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)    
    # wrap around environment for rsl-rl
    env = RslRlVecEnvWrapper(env)

    ppo_runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=None, device=agent_cfg.device)
    ppo_runner.load(resume_path)
    # obtain the trained policy for inference
    policy = ppo_runner.get_inference_policy(device=env.unwrapped.device)
    
    # reset environment
    obs, _ = env.get_observations()
    timestep = 0
    # simulate environment
    while simulation_app.is_running():
        # run everything in inference mode
        with torch.inference_mode():
            # agent stepping
            actions = policy(obs)
            # env stepping
            obs, _, _, _ = env.step(actions)

    # close the simulator
    env.close()
~~~

&nbsp;
## 4.2 manager_based_env.py

Starting from the source code of `RslRlVecEnvWrapper`, we traced back to the source code of `ManagerBasedEnv`. Over there, we found that it was `ActionManager` who took charge of executing `env.step(actions)`. 

~~~
from omni.isaac.lab.managers import ActionManager, EventManager, ObservationManager
from .manager_based_env_cfg import ManagerBasedEnvCfg

class ManagerBasedEnv:
    def step(self, action: torch.Tensor) -> tuple[VecEnvObs, dict]:
        # process actions
        self.action_manager.process_action(action.to(self.device))
        
        # perform physics stepping
        for _ in range(self.cfg.decimation):
            # set actions into buffers
            self.action_manager.apply_action()            
~~~

The source code of `ManagerBasedEnv` locates at `/home/robot/IsaacLab/source/extensions/omni.isaac.lab/omni/isaac/lab/envs/manager_based_env.py`

&nbsp;
## 4.3 action_manager.py

~~~
class ActionTerm(ManagerTermBase):
    def __init__(self, cfg: ActionTermCfg, env: ManagerBasedEnv):
        """Initialize the action term.
        Args:
            cfg: The configuration object.
            env: The environment instance.
        """
        # call the base class constructor
        super().__init__(cfg, env)
        # parse config to obtain asset to which the term is applied
        self._asset: AssetBase = self._env.scene[self.cfg.asset_name]
        ...
        
    @abstractmethod
    def process_actions(self, actions: torch.Tensor):
        """Processes the actions sent to the environment.
        Args:
            actions: The actions to process.
        """
        raise NotImplementedError       

    @abstractmethod
    def apply_actions(self):
        """Applies the actions to the asset managed by the term.
        Note:
            This is called at every simulation step by the manager.
        """
        raise NotImplementedError    


class ActionManager(ManagerBase):  
    def __init__(self, cfg: object, env: ManagerBasedEnv):
        """Initialize the action manager.

        Args:
            cfg: The configuration object or dictionary (``dict[str, ActionTermCfg]``).
            env: The environment instance.
        """
        ...
         
    def process_action(self, action: torch.Tensor):
        """Processes the actions sent to the environment.
        Note:
            This function should be called once per environment step.
        Args:
            action: The actions to process.
        """
        ...
        # split the actions and apply to each tensor
        idx = 0
        for term in self._terms.values():
            term_actions = action[:, idx : idx + term.action_dim]
            term.process_actions(term_actions)
            idx += term.action_dim

    def apply_action(self) -> None:
        """Applies the actions to the environment/simulation.

        Note:
            This should be called at every simulation step.
        """
        for term in self._terms.values():
            term.apply_actions()         
~~~

1. The source code of `action_manager.py` locates at `/home/robot/IsaacLab/source/extensions/omni.isaac.lab/omni/isaac/lab/managers/action_manager.py`,

2. Both `process_actions()` and `apply_actions()` are of abstract method, that means their implementation is actually done by `ActionManager`'s descendant classes.

3. The initialization of instances of `ActionTerm` and `ActionManager` rely on `cfg`, and `cfg` comes from `play.py`.

~~~
from omni.isaac.lab_tasks.utils import get_checkpoint_path, parse_env_cfg

def main():
    """Play with RSL-RL agent."""
    # parse configuration
    env_cfg = parse_env_cfg(
        args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric
    )
    agent_cfg: RslRlOnPolicyRunnerCfg = cli_args.parse_rsl_rl_cfg(args_cli.task, args_cli)

    # create isaac environment
    # args_cli.task == "Isaac-Velocity-Flat-Unitree-Go2-v0"
    env = gym.make(args_cli.task, cfg=env_cfg, render_mode="rgb_array" if args_cli.video else None)
~~~

&nbsp;
## 4.4 go2/__init__.py

As mentioned above, the initialization of `ActionTerm` and `ActionManager` instances relies on `cfg`, and `cfg` comes from `play.py`, 

~~~
from omni.isaac.lab_tasks.utils import get_checkpoint_path, parse_env_cfg

env_cfg = parse_env_cfg(
    args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric
)
~~~

In the code above, `args_cli.task` indicates that `env_cfg` is decided by the task name, and the task name comes from the CLI command parameter, for example `Isaac-Velocity-Flat-Unitree-Go2-v0`. 

~~~
$ cd ${HOME}/IsaacLab

$ ./isaaclab.sh -p source/standalone/workflows/rsl_rl/play.py \
      --task Isaac-Velocity-Flat-Unitree-Go2-v0 --num_envs 1
~~~

Referring to the Isaac Lab's tutorial "[Registering an Environment](https://isaac-sim.github.io/IsaacLab/source/tutorials/03_envs/register_rl_env_gym.html#manager-based-environments)", 

> For manager-based environments, the following shows the registration call for the cartpole environment in the omni.isaac.lab_tasks.manager_based.classic.cartpole sub-package. 

also according to the source code of `parse_env_cfg.py`, the task `Isaac-Velocity-Flat-Unitree-Go2-v0` is registered in `go2/__init__.py`. 

~~~
gym.register(
    id="Isaac-Velocity-Flat-Unitree-Go2-v0",
    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
    disable_env_checker=True,
    kwargs={
        "env_cfg_entry_point": flat_env_cfg.UnitreeGo2FlatEnvCfg,
        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:UnitreeGo2FlatPPORunnerCfg",
        "skrl_cfg_entry_point": f"{agents.__name__}:skrl_flat_ppo_cfg.yaml",
    },
)
~~~

1. The source code of `go2/__init__.py` locates at `/home/robot/IsaacLab/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/config/go2/__init__.py`

2. Based on the registration information of `env`, the content of `env` is decided by `env_cfg_entry_point`, in our case `env_cfg_entry_point` is `flat_env_cfg.UnitreeGo2FlatEnvCfg`.


&nbsp;
## 4.5 go2/rough_env_cfg.py

According to the source code of `UnitreeGo2FlatEnvCfg`, 

`/home/robot/IsaacLab/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/config/go2/flat_env_cfg.py`, 

the super class of `UnitreeGo2FlatEnvCfg` is `UnitreeGo2RoughEnvCfg`. 

Therefore, we review the source code of `UnitreeGo2RoughEnvCfg`, 

~~~
from omni.isaac.lab_tasks.manager_based.locomotion.velocity.velocity_env_cfg import LocomotionVelocityRoughEnvCfg
from omni.isaac.lab_assets.unitree import UNITREE_GO2_CFG

@configclass
class UnitreeGo2RoughEnvCfg(LocomotionVelocityRoughEnvCfg):
    def __post_init__(self):
        # post init of parent
        super().__post_init__()

        self.scene.robot = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
        ...
        
        # reduce action scale
        self.actions.joint_pos.scale = 0.25

        # event
        self.events.push_robot = None
        ...

        # rewards
        self.rewards.feet_air_time.params["sensor_cfg"].body_names = ".*_foot"
        ...

        # terminations
        self.terminations.base_contact.params["sensor_cfg"].body_names = "base"
~~~

1. The source code of `UnitreeGo2RoughEnvCfg` locates at `/home/robot/IsaacLab/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/config/go2/rough_env_cfg.py`

2. In the source code of `UnitreeGo2RoughEnvCfg`, `self.scene.robot` points to the 3D model of unitree go2 robotic dog, hence, we guess that `unitree.UNITREE_GO2_CFG` might have the physical parameters of the unitree go2 dog.

3. The super class of `UnitreeGo2RoughEnvCfg` is `LocomotionVelocityRoughEnvCfg`, we guess that the implementation of `ActionManager` might reside over there. 


&nbsp;
## 4.6 velocity_env_cfg.py

The source code of `LocomotionVelocityRoughEnvCfg` is `velocity_env_cfg.py`, 

~~~
import omni.isaac.lab_tasks.manager_based.locomotion.velocity.mdp as mdp

@configclass
class ActionsCfg:
    """Action specifications for the MDP."""
    joint_pos = mdp.JointPositionActionCfg(asset_name="robot", joint_names=[".*"], scale=0.5, use_default_offset=True)

@configclass
class LocomotionVelocityRoughEnvCfg(ManagerBasedRLEnvCfg):
    """Configuration for the locomotion velocity-tracking environment."""
    # Scene settings
    scene: MySceneCfg = MySceneCfg(num_envs=4096, env_spacing=2.5)
    # Basic settings
    observations: ObservationsCfg = ObservationsCfg()
    actions: ActionsCfg = ActionsCfg()
    commands: CommandsCfg = CommandsCfg()
    # MDP settings
    rewards: RewardsCfg = RewardsCfg()
    terminations: TerminationsCfg = TerminationsCfg()
    events: EventCfg = EventCfg()
    curriculum: CurriculumCfg = CurriculumCfg()
    ...
~~~

1. `velocity_env_cfg.py` locates at `/home/robot/IsaacLab/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py`.

2. The content of `actions` is decided by `ActionsCfg`.

3. The content of `ActionsCfg` is decided by `mdp.JointPositionActionCfg`.

From the name of `mdp.JointPositionActionCfg`, we guess that `action` tensor's 12 elements might correspond to the joint positions of the robotic dog. Each dog has 4 legs, and each leg has 3 joints, the body joint, the thigh joint, and the calf joint. 

We will verify our guess later on. 

&nbsp;
## 4.7 mdp/actions/joint_actions.py

Starting from `mdp/__init__.py`, we traced back its ancestor classes, until `joint_actions.py`,

~~~
from omni.isaac.lab.assets.articulation import Articulation

class JointAction(ActionTerm):

    cfg: actions_cfg.JointActionCfg
    """The configuration of the action term."""
    _asset: Articulation
    """The articulation asset on which the action term is applied."""

    def __init__(self, cfg: actions_cfg.JointActionCfg, env: ManagerBasedEnv) -> None:
        # initialize the action term
        super().__init__(cfg, env)

        # resolve the joints over which the action term is applied
        self._joint_ids, self._joint_names = self._asset.find_joints(
            self.cfg.joint_names, 
            preserve_order=self.cfg.preserve_order
        )
        

class JointPositionAction(JointAction):
    """Joint action term that applies the processed actions to the articulation's joints as position commands."""

    cfg: actions_cfg.JointPositionActionCfg
    """The configuration of the action term."""

    def __init__(self, cfg: actions_cfg.JointPositionActionCfg, env: ManagerBasedEnv):
        # initialize the action term
        super().__init__(cfg, env)
        # use default joint positions as offset
        if cfg.use_default_offset:
            self._offset = self._asset.data.default_joint_pos[:, self._joint_ids].clone()

    def apply_actions(self):
        # set position targets
        self._asset.set_joint_position_target(
            self.processed_actions, 
            joint_ids=self._joint_ids
        )
   
        
class JointVelocityAction(JointAction):
    ...

class JointEffortAction(JointAction):
    ...
~~~

1. The source code of `joint_actions.py` locates at `/home/robot/IsaacLab/source/extensions/omni.isaac.lab/omni/isaac/lab/envs/mdp/actions/joint_actions.py`.

2. `joint_actions.py` implements `JointPositionAction`, as well as `JointVelocityAction` and `JointEffortAction`, even though we don't use the later two for the time being.

3. The implementation of `JointPositionAction.apply_actions()` is quite straightforward.

    The raw inputs are preprocessed into `processed_actions`, and each action of `processed_actions` is applied to each joint `joint_id`.

4. As mentioned above, `action` tensor consists of 12 elements, at the previous moment, we didn't know which element corresponds to which physical parameter of the robotic dog.

    But now, we know for sure that the 12 elements of `action` tensor correspond to the joints of the robotic dog's 4 legs, i.e `joint_id`s. 

    The remaining job is to find out which `joint_id` corresponds to which leg joint specifically. 


&nbsp;
## 4.8 unitree.py

In the previous section, we mentioned that, 

> In the source code of `UnitreeGo2RoughEnvCfg`, `self.scene.robot` points to the 3D model of unitree go2 robotic dog,
> hence, we guess that `unitree.UNITREE_GO2_CFG` might have the physical parameters of the unitree go2 dog.

~~~
from omni.isaac.lab_assets.unitree import UNITREE_GO2_CFG

@configclass
class UnitreeGo2RoughEnvCfg(LocomotionVelocityRoughEnvCfg):

    def __post_init__(self):
        self.scene.robot = UNITREE_GO2_CFG.replace(prim_path="{ENV_REGEX_NS}/Robot")
        ...
~~~

Review the source code of `unitree.py`, 

~~~
"""Configuration for Unitree robots.

The following configurations are available:

* :obj:`UNITREE_A1_CFG`: Unitree A1 robot with DC motor model for the legs
* :obj:`UNITREE_GO1_CFG`: Unitree Go1 robot with actuator net model for the legs
* :obj:`UNITREE_GO2_CFG`: Unitree Go2 robot with DC motor model for the legs
* :obj:`H1_CFG`: H1 humanoid robot
* :obj:`G1_CFG`: G1 humanoid robot

Reference: https://github.com/unitreerobotics/unitree_ros
# 这里只有 Go1 的 config，没有 Go2 的 config
"""

import omni.isaac.lab.sim as sim_utils
from omni.isaac.lab.actuators import ActuatorNetMLPCfg, DCMotorCfg, ImplicitActuatorCfg
from omni.isaac.lab.assets.articulation import ArticulationCfg
from omni.isaac.lab.utils.assets import ISAACLAB_NUCLEUS_DIR

UNITREE_GO2_CFG = ArticulationCfg(
    init_state=ArticulationCfg.InitialStateCfg(
        pos=(0.0, 0.0, 0.4),
        joint_pos={
            ".*L_hip_joint": 0.1,
            ".*R_hip_joint": -0.1,
            "F[L,R]_thigh_joint": 0.8,
            "R[L,R]_thigh_joint": 1.0,
            ".*_calf_joint": -1.5,
        },
        joint_vel={".*": 0.0},
    ),
    ...
)
"""Configuration of Unitree Go2 using DC-Motor actuator model."""
~~~

1. The source code of `unitree.py` locates at `/home/robot/IsaacLab/source/extensions/omni.isaac.lab_assets/omni/isaac/lab_assets/unitree.py`.

2. `joint_id`s，in sequence, correspond to these dog leg joints,

~~~
(1) FL_hip_joint, (2) RL_hip_joint, (3) FR_hip_joint, (4) RR_hip_joint,
(5) FL_thigh_joint, (6) FR_thigh_joint, (7) RL_thigh_joint, (8) RR_thigh_joint,
(9) FL_calf_joint, (10) FR_calf_joint, (11) RL_calf_joint, (12) RR_calf_joint.
~~~

In short, the 3'rd sub-task, to understand the physical meaning of each element of actions, is done. 


&nbsp;
# 5. Reinforcement learning-based motion model

## 5.1 agent_cfg

In the previous section, we analyzed the source code of `play.py`, and now we add `print()` into it, to display the content of `agent_cfg`, 

~~~
from rsl_rl.runners import OnPolicyRunner
from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
    RslRlOnPolicyRunnerCfg,
    RslRlVecEnvWrapper,
    ...
)

def main():
    """Play with RSL-RL agent."""
    # parse configuration
    env_cfg = parse_env_cfg(
        args_cli.task, device=args_cli.device, num_envs=args_cli.num_envs, use_fabric=not args_cli.disable_fabric
    )
    agent_cfg: RslRlOnPolicyRunnerCfg = cli_args.parse_rsl_rl_cfg(args_cli.task, args_cli)

    # resume_path: /home/robot/IsaacLab/logs/rsl_rl/unitree_go2_flat/2024-09-01_16-32-19/model_299.pt
    print(f"[INFO]: Loading model checkpoint from resume_path: {resume_path}")
    # load previously trained model
    print(f"\n 【Kan】agent_cfg.to_dict(): '{agent_cfg.to_dict()}'\n")
    
    ppo_runner = OnPolicyRunner(env, agent_cfg.to_dict(), log_dir=None, device=agent_cfg.device)
    ppo_runner.load(resume_path)
    # obtain the trained policy for inference
    policy = ppo_runner.get_inference_policy(device=env.unwrapped.device)
    
    # reset environment
    obs, _ = env.get_observations()
    timestep = 0
    # simulate environment
    while simulation_app.is_running():
        # run everything in inference mode
        with torch.inference_mode():
            # agent stepping
            actions = policy(obs)
            # env stepping
            obs, _, _, _ = env.step(actions)
            
    # close the simulator
    env.close()
~~~

- play.py 具体位置是，
  ${HOME}/IsaacLab/source/standalone/workflows/rsl_rl/play.py
- 运行 play.py 后，显示出了 agent_cfg 的结果如下，

1. The source code of `play.py` locates at `/home/robot/IsaacLab/source/standalone/workflows/rsl_rl/play.py`. 

2. The following is the execution result of `play.py`,

~~~
$ ./isaaclab.sh -p source/standalone/workflows/rsl_rl/play.py --task Isaac-Velocity-Flat-Unitree-Go2-Play-v0 --num_envs 1

[INFO]: Loading model checkpoint from resume_path: /home/robot/IsaacLab/logs/rsl_rl/unitree_go2_flat/2024-09-01_16-32-19/model_299.pt

【Kan】agent_cfg.to_dict(): '{  
     'seed': 42, 
     'device': 'cuda:0', 
     'num_steps_per_env': 24, 
     'max_iterations': 300, 
     'empirical_normalization': False, 
     'policy': {'class_name': 'ActorCritic', 
                'init_noise_std': 1.0, 
                'actor_hidden_dims': [128, 128, 128], 
                'critic_hidden_dims': [128, 128, 128], 
                'activation': 'elu'}, 
     'algorithm': {'class_name': 'PPO', 
                   'value_loss_coef': 1.0, 
                   'use_clipped_value_loss': True, 
                   'clip_param': 0.2, 
                   'entropy_coef': 0.01, 
                   'num_learning_epochs': 5, 
                   'num_mini_batches': 4, 
                   'learning_rate': 0.001, 
                   'schedule': 'adaptive', 
                   'gamma': 0.99, 
                   'lam': 0.95, 
                   'desired_kl': 0.01, 
                   'max_grad_norm': 1.0}, 
     'save_interval': 50, 
     'experiment_name': 'unitree_go2_flat', 
     'run_name': '', 
     'logger': 'tensorboard', 
     'neptune_project': 'isaaclab', 
     'wandb_project': 'isaaclab', 
     'resume': False, 
     'load_run': '.*', 
     'load_checkpoint': 'model_.*.pt'
   }'
~~~


&nbsp;
## 5.2 rsl_rl_ppo_cfg.py

In the previous section, we analyzed the source code of `go2/__init__.py`, 

~~~
gym.register(
    id="Isaac-Velocity-Flat-Unitree-Go2-v0",
    entry_point="omni.isaac.lab.envs:ManagerBasedRLEnv",
    disable_env_checker=True,
    kwargs={
        "env_cfg_entry_point": flat_env_cfg.UnitreeGo2FlatEnvCfg,
        "rsl_rl_cfg_entry_point": f"{agents.__name__}.rsl_rl_ppo_cfg:UnitreeGo2FlatPPORunnerCfg",
        "skrl_cfg_entry_point": f"{agents.__name__}:skrl_flat_ppo_cfg.yaml",
    },
)
~~~

1. The source code of `go2/__init__.py` locates at `/home/robot/IsaacLab/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/config/go2/__init__.py`.

2. From the registration information of `env`, we know that the content of the reinforcement learning-based motion model is decided by `rsl_rl_cfg_entry_point`, in our case, the entry point is `rsl_rl_ppo_cfg:UnitreeGo2FlatPPORunnerCfg`. 

&nbsp;
Following is the source code of `rsl_rl_ppo_cfg:UnitreeGo2FlatPPORunnerCfg`, 

~~~
from omni.isaac.lab.utils import configclass
from omni.isaac.lab_tasks.utils.wrappers.rsl_rl import (
    RslRlOnPolicyRunnerCfg,
    RslRlPpoActorCriticCfg,
    RslRlPpoAlgorithmCfg,
)

@configclass
class UnitreeGo2RoughPPORunnerCfg(RslRlOnPolicyRunnerCfg):
    num_steps_per_env = 24
    max_iterations = 1500
    save_interval = 50
    experiment_name = "unitree_go2_rough"
    empirical_normalization = False
    policy = RslRlPpoActorCriticCfg(
        init_noise_std=1.0,
        actor_hidden_dims=[512, 256, 128],
        critic_hidden_dims=[512, 256, 128],
        activation="elu",
    )
    algorithm = RslRlPpoAlgorithmCfg(
        value_loss_coef=1.0,
        use_clipped_value_loss=True,
        clip_param=0.2,
        entropy_coef=0.01,
        num_learning_epochs=5,
        num_mini_batches=4,
        learning_rate=1.0e-3,
        schedule="adaptive",
        gamma=0.99,
        lam=0.95,
        desired_kl=0.01,
        max_grad_norm=1.0,
    )

@configclass
class UnitreeGo2FlatPPORunnerCfg(UnitreeGo2RoughPPORunnerCfg):
    def __post_init__(self):
        super().__post_init__()

        self.max_iterations = 300
        self.experiment_name = "unitree_go2_flat"
        self.policy.actor_hidden_dims = [128, 128, 128]
        self.policy.critic_hidden_dims = [128, 128, 128]
~~~

1. The source code of `rsl_rl_ppo_cfg:UnitreeGo2FlatPPORunnerCfg` locates at `/home/robot/IsaacLab/source/extensions/omni.isaac.lab_tasks/omni/isaac/lab_tasks/manager_based/locomotion/velocity/config/go2/agents/rsl_rl_ppo_cfg.py`

2. The content of `rsl_rl_ppo_cfg.py` is equivalent to the content of `agent_cfg` which was displayed in the previous section.

   
&nbsp;
## 5.3 setup.py

In the source code of `play.py`, it `import` the `rsl_rl` package. 

~~~
from rsl_rl.runners import OnPolicyRunner
~~~

Where does the `rsl_rl` package come from? We review the source code of `setup.py`. 

~~~
# Extra dependencies for RL agents
EXTRAS_REQUIRE = {
    "sb3": ["stable-baselines3>=2.1"],
    "skrl": ["skrl>=1.2.0"],
    "rl-games": ["rl-games==1.6.1", "gym"],  # rl-games still needs gym :(
    "rsl-rl": ["rsl-rl@git+https://github.com/leggedrobotics/rsl_rl.git"],
    "robomimic": [],
}

# Add the names with hyphens as aliases for convenience
EXTRAS_REQUIRE["rl_games"] = EXTRAS_REQUIRE["rl-games"]
EXTRAS_REQUIRE["rsl_rl"] = EXTRAS_REQUIRE["rsl-rl"]
~~~

- setup.py 的具体位置是，
  ${HOME}/IsaacLab/source/extensions/omni.isaac.lab_tasks/setup.py
- rsl_rl package 是从 rsl_rl github 上下载的，网址是，
  https://github.com/leggedrobotics/rsl_rl/
- 我们需要在 Isaac Lab 系统外，重新安装 rsl_rl package，因为后续工作，将脱离 Isaac Lab 系统，

1. The source code of `setup.py` locates at `/home/robot/IsaacLab/source/extensions/omni.isaac.lab_tasks/setup.py`

2. The `rsl_rl` package is downloaded from rsl_rl's github page, and its URL is `https://github.com/leggedrobotics/rsl_rl`

3. We need to install the `rsl_rl` package outside of Isaac Lab system.

    Because later on we will run the `rsl_rl` package on the body of the robotic dog, and we will not install Isaac Lab system on the dog body.

~~~
$ git clone https://github.com/leggedrobotics/rsl_rl
$ cd rsl_rl
$ pip install -e .
~~~


&nbsp;
# 6. env portable to the robotic dog's body

As mentioned in the first section of this article, 

> Our ultimate goal is to train the robotic dog in a simulated environment to generate a checkpoint of the motion model,
> and then apply the motion model loaded with the checkpoint to the physical robotic dog,
> to enable it to walk stably in the real world, in short, our goal is "sim-to-real".
>
> In the previous article, we trained a motion model checkpoint for the robotic dog in the Isaac Lab simulation environment.
> Our next task is divided into two major steps:
>
> 1. Referring to [the official tutorial of Gymnasium](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/),
>    we will write python programs to implement our own environment `env`, where the observation input and the action output do not come from the Isaac Lab simulation environment,
>    but from the physical Unitree-Go2 robotic dog.
>    
> 2. Referring to [the official tutorial of the Unitree robotic dog Go2](https://support.unitree.com/home/en/developer/Basic_motion_control),
>    we will call the basic motion control APIs to receive the real-time motion status of the robotic dog, and then convert it into observations for the `env`.
>    After then, we will use these observations as inputs for the motion model, after that convert the outputs generated by the motion model into actions,
>    to control the robotic dog's next movements in real time.
> 
> Since there are quite some content involved in these 2 tasks, we focus the subject of this article on the implementation of the first step mentioned above:
>  to implement our own environment `env`.

After finishing the research on `observations` and `actions`, and installing the `rsl_rl` package outside of the Isaac Lab, now, we can implement our own `env` that is portable to the robotic dog's body. 

1. go2_real_env.py

    We implemented an `env`, `go2_real_env.py`, and will port it to a unitree go2 robotic dog's body, enabling the dog to walk stably in the real world.

    Inside the source code of `go2_real_env.py`, `self.rl_cfg` is used for the configuration of the reinforcement learning-based motion model.

    The source code of `go2_real_env.py` locates at [the /asset sub-directory of this github repo](./S04E03_asset/go2_real_env.py). 

2. rsl_rl_ppo_cfg.json

    As mentioned above, `self.rl_cfg` is used for the configuration of RL motion model, and its content comes from `rl_cfg_file`, which is a json file.

    We create [a sample RL configuration json file](./S04E03_asset/rsl_rl_ppo_cfg.json) in this repo.

    The content of `rsl_rl_ppo_cfg.json` is copied and pasted from the `agent_cfg` and `rsl_rl_ppo_cfg.py` as mentioned in the previous sections. 

3. go2_play.py

    Our `env` differs from Isaac Lab's `env` in that our `env` doesn't contain simulation, in other words, our `env` doesn't offer any simulation functionality related to Isaac Sim.

    However, if the observation inputs are exactly the same, the action outputs of our `env` must be exactly the same as Isaac Lab's `env`.

    We use `go2_play.py` to verify the correctness of our `env`, by comparing its inputs and outputs with those of Isaac Lab's inputs and outputs.

    [`go2_play.py`](./S04E03_asset/go2_play.py) locates at the `S04E03_asset` sub-directory of this repo.

&nbsp;
We created a file directory in our GPU computer, and downed `go2_play.py`，`rsl_rl_ppo_cfg.json`，and `go2_play.py` into this directory.  

After that, we executed `go2_play.py`, and the execution result displayed that, when inputing the exactly same observations to both our `go_env` and Isaac Lab's `env`, their output actions. 

As we anticipated, their action outputs are exactly the same. 

~~~
$ /home/robot/IsaacLab
$ python3 unitree/go2_play.py 

 >> Go2RealEnv() 

【Kan】rl_cfg: {'seed': 42, 'device': 'cuda:0', 'num_steps_per_env': 24, 'max_iterations': 300, 'empirical_normalization': False, 'policy': {'class_name': 'ActorCritic', 'init_noise_std': 1.0, 'actor_hidden_dims': [128, 128, 128], 'critic_hidden_dims': [128, 128, 128], 'activation': 'elu'}, 'algorithm': {'class_name': 'PPO', 'value_loss_coef': 1.0, 'use_clipped_value_loss': True, 'clip_param': 0.2, 'entropy_coef': 0.01, 'num_learning_epochs': 5, 'num_mini_batches': 4, 'learning_rate': 0.001, 'schedule': 'adaptive', 'gamma': 0.99, 'lam': 0.95, 'desired_kl': 0.01, 'max_grad_norm': 1.0}, 'save_interval': 50, 'experiment_name': 'unitree_go2_flat', 'run_name': '', 'logger': 'tensorboard', 'neptune_project': 'isaaclab', 'wandb_project': 'isaaclab', 'resume': False, 'load_run': '.*', 'load_checkpoint': 'model_.*.pt'} 

【Kan】observation_space: 'Dict('policy': Box(-inf, inf, (1, 48), float64))'
【Kan】action_space: 'Box(-inf, inf, (1, 12), float64)' 

【Kan】go2_env.get_observations(): (tensor([[-0.5578, -0.8615, -0.0395, -0.5672,  0.1676, -0.0858,  0.0587,  0.0334,
         -1.0118, -0.5736, -0.8130,  0.0198, -0.1076,  0.2249,  0.1174, -0.1215,
          0.4330, -0.4039, -0.0043, -0.2249, -0.3630, -0.2957, -0.2046, -0.2444,
         -3.6872,  0.8345,  0.9783,  3.4316,  0.9197,  0.7776, -0.0279, -1.5330,
         -2.3675,  0.7889, -1.4047, -3.2064, -1.2007,  0.1980,  0.1681,  0.8183,
          1.4110, -0.2133,  0.4731, -1.4501, -1.1923, -0.0632, -0.5582,  0.3421]]), {'observations': {'shape': [48, 1], 'base_lin_vel': [[-0.5578, -0.8615, -0.0395]], 'base_ang_vel': [[-0.5672, 0.1676, -0.0858]], 'projected_gravity': [[0.0587, 0.0334, -1.0118]], 'velocity_commands': [[-0.5736, -0.813, 0.0198]], 'joint_pos': [[-0.1076, 0.2249, 0.1174, -0.1215, 0.433, -0.4039, -0.0043, -0.2249, -0.363, -0.2957, -0.2046, -0.2444]], 'joint_vel': [[-3.6872, 0.8345, 0.9783, 3.4316, 0.9197, 0.7776, -0.0279, -1.533, -2.3675, 0.7889, -1.4047, -3.2064]], 'actions': [[-1.2007, 0.198, 0.1681, 0.8183, 1.411, -0.2133, 0.4731, -1.4501, -1.1923, -0.0632, -0.5582, 0.3421]]}}) 

[INFO]: Loading model checkpoint from resume_path: /home/robot/IsaacLab/logs/rsl_rl/unitree_go2_flat/2024-09-01_16-32-19/model_299.pt
Successfully load the model with checkpoint.

【Kan】go2_actions: tensor([[-0.6142, -0.6126, -0.3642,  1.7715,  0.4137,  0.4264,  1.3493, -1.4573,
          0.0720, -0.4770, -0.8688,  0.3635]]) 

【Kan】obs: tensor([[-0.5578, -0.8615, -0.0395, -0.5672,  0.1676, -0.0858,  0.0587,  0.0334,
         -1.0118, -0.5736, -0.8130,  0.0198, -0.1076,  0.2249,  0.1174, -0.1215,
          0.4330, -0.4039, -0.0043, -0.2249, -0.3630, -0.2957, -0.2046, -0.2444,
         -3.6872,  0.8345,  0.9783,  3.4316,  0.9197,  0.7776, -0.0279, -1.5330,
         -2.3675,  0.7889, -1.4047, -3.2064, -1.2007,  0.1980,  0.1681,  0.8183,
          1.4110, -0.2133,  0.4731, -1.4501, -1.1923, -0.0632, -0.5582,  0.3421]]) 

【Kan】go2_actions: tensor([[-0.6142, -0.6126, -0.3642,  1.7715,  0.4137,  0.4264,  1.3493, -1.4573,
          0.0720, -0.4770, -0.8688,  0.3635]]) 

【Kan】obs: tensor([[-0.5578, -0.8615, -0.0395, -0.5672,  0.1676, -0.0858,  0.0587,  0.0334,
         -1.0118, -0.5736, -0.8130,  0.0198, -0.1076,  0.2249,  0.1174, -0.1215,
          0.4330, -0.4039, -0.0043, -0.2249, -0.3630, -0.2957, -0.2046, -0.2444,
         -3.6872,  0.8345,  0.9783,  3.4316,  0.9197,  0.7776, -0.0279, -1.5330,
         -2.3675,  0.7889, -1.4047, -3.2064, -1.2007,  0.1980,  0.1681,  0.8183,
          1.4110, -0.2133,  0.4731, -1.4501, -1.1923, -0.0632, -0.5582,  0.3421]]) 
 ...
 
【Kan】go2_actions: tensor([[-0.6142, -0.6126, -0.3642,  1.7715,  0.4137,  0.4264,  1.3493, -1.4573,
          0.0720, -0.4770, -0.8688,  0.3635]]) 

【Kan】obs: tensor([[-0.5578, -0.8615, -0.0395, -0.5672,  0.1676, -0.0858,  0.0587,  0.0334,
         -1.0118, -0.5736, -0.8130,  0.0198, -0.1076,  0.2249,  0.1174, -0.1215,
          0.4330, -0.4039, -0.0043, -0.2249, -0.3630, -0.2957, -0.2046, -0.2444,
         -3.6872,  0.8345,  0.9783,  3.4316,  0.9197,  0.7776, -0.0279, -1.5330,
         -2.3675,  0.7889, -1.4047, -3.2064, -1.2007,  0.1980,  0.1681,  0.8183,
          1.4110, -0.2133,  0.4731, -1.4501, -1.1923, -0.0632, -0.5582,  0.3421]]) 

[INFO] go2_real_env is closed.
~~~

